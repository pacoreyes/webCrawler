<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta charset="UTF-8"/>
  <title>About | Web Crawler</title>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/styles.css') }}"/>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/normalize.css') }}"/>
  <link rel="shortcut icon" href="{{ url_for('static', filename='images/favicon.svg') }}"/>
  <meta name="robots" content="noindex"/>
  <meta name="googlebot" content="noindex"/>
  <script type="module">
    import {toggleNavigationMenuVisibility} from "../static/js/lib/utils.mjs";

    toggleNavigationMenuVisibility();
  </script>
</head>
<body>
{% include 'partials/header.html' %}
<main>
  <section>
    <h1>About webCrawler</h1>
    <p>This Python application is a web crawler that systematically browses websites, processing the URLs and storing
      relevant data. It's particularly focused on extracting information related to politicians from the sites it
      visits.</p>
  </section>

  <section>
    <h2>Features</h2>
    <article>
      <p>The web crawler provides the following key features:</p>
      <ul>
        <li><strong>Website Crawling</strong>: It visits each URL of a website and processes the content to extract
          useful data.
        </li>
        <li><strong>Customizable User Agents</strong>: Randomly selects user agents from a list for each request. This
          is done to prevent blocking by websites that limit requests from the same user agent.
        </li>
        <li><strong>URL Filtering</strong>: It filters out suspicious URLs and URLs that are not allowed by
          `robots.txt` of the website. It also ensures that it only visits new URLs and does not revisit already
          visited ones.
        </li>
        <li><strong>Data Extraction</strong>: Extracts page content, including the page title and text within
          paragraph tags.
        </li>
        <li><strong>Politician Information Extraction</strong>: It specifically looks for politician names in the
          title of each page visited and classifies the page as potentially useful if it contains a politician's name.
        </li>
        <li><strong>Data Storage</strong>: Stores data in a SQLite database and Firestore. It saves all URLs visited,
          but only includes additional data for potentially useful pages (those that contain politician names).
        </li>
        <li><strong>Error Handling</strong>: The application handles various exceptions that may arise during the web
          crawling process, such as timeouts and WebDriver exceptions.
        </li>
      </ul>
    </article>
  </section>

  <section>
    <h2>Authors</h2>
    <article>
      <p>abc</p>
    </article>
  </section>
</main>
{% include 'partials/footer.html' %}
</body>
</html>
